---
title: LLM是如何读懂文字的
toc: true
date: 2026-02-04 17:06:31
categories: AI
tags: AI
---


> 本文深入浅出地解析了大语言模型（LLM）如何将人类语言转换为数学运算，再将计算结果转换回文字的完整过程。从分词器（Tokenizer）将文字切分为整数ID，到嵌入层（Embedding Layer）将ID映射为高维语义向量，再经过位置编码（Positional Encoding）添加顺序信息，然后通过几十层Transformer进行矩阵计算和语义理解，最后通过反向映射（Unembedding）和采样策略生成输出文字。

---

## 分词器（Tokenizer）— 查字典，把字变整数
在你按下回车键的瞬间，文字并没有直接进入模型的大脑（神经网络），而是先被送到了一个名为 **Tokenizer（分词器）** 的组件中。

Tokenizer 不是神经网络，它更像是一个静态的**超级字典**。

1.  **切分：** 输入句子 `我爱AI`。
2.  **查表：** Tokenizer 会查阅一个巨大的词表（Vocabulary），找到每个字对应的**唯一编号（ID）**。
    *   "我" -> ID: `254`
    *   "爱" -> ID: `3301`
    *   "AI" -> ID: `1209`
3.  **输出：** 此时，文字变成了一个整数列表（Tensor）：`[254, 3301, 1209]`。

> **注意：** 这一步只是查表，没有任何复杂的矩阵运算，也没有语义理解。电脑只知道现在来了三个代号。

### 不同的分词算法

实际上，不同的模型使用不同的分词算法：

*   **BPE (Byte Pair Encoding)：** GPT系列使用的算法，会把高频出现的字符组合合并成一个token。比如"ing"、"ed"可能是独立的token。
*   **WordPiece：** BERT使用的算法，类似BPE，但使用不同的合并策略。
*   **SentencePiece：** 一些多语言模型使用，可以直接处理原始文本，不需要预先分词。

这就是为什么相同的句子在GPT-4和Claude中，token数量可能不一样——它们用的字典不同！

> **关键要点：训练与推理的一致性**
> 
> 这里有一个至关重要的技术细节：**模型在对话（推理）时使用的Tokenizer，必须和训练时使用的Tokenizer完全相同**。
> 
> 想象一下，训练时"AI"这个词被分配了ID 1209，Embedding矩阵的第1209行就学习到了"AI"的语义。如果推理时用了不同的Tokenizer，"AI"可能被分配成ID 5678，模型就会去查第5678行——那里存的可能是"香蕉"的语义！整个系统就乱套了。
> 
> 所以每个LLM都会把Tokenizer和模型权重**绑定发布**，确保你下载GPT-4模型时，同时拿到的是训练GPT-4时用的那个专属词表和分词规则。这就像是模型的"专属字典"，不能随意更换。

### 特殊标记（Special Tokens）

除了普通文字，Tokenizer还会添加一些特殊标记：

*   **`[CLS]`：** 分类标记（BERT），表示句子开始
*   **`[SEP]`：** 分隔标记，用于分隔不同的句子
*   **`[PAD]`：** 填充标记，用于对齐批次处理中不同长度的输入
*   **`[MASK]`：** 掩码标记（BERT），用于训练时遮蔽某些词
*   **`<|endoftext|>`：** GPT中表示文本结束

这些特殊标记就像是给模型的"指令符号"，告诉它这段文字的结构。

---

## Embedding Layer（嵌入层）— 把整数变向量

**模型内部自带了一个巨大的 Embedding 矩阵**。外部的 Embedding Model 实际上就是把 Model 的前几层（Tokenization + Embedding Layer）单独拿出来用而已。Model 自己体内天生就"内置"了这个转换器。

虽然你没有使用外部的 embedding model，但 Base Model 的架构中，**第一层（Layer 0）** 永远是一个巨大的查找表矩阵。

*   **矩阵长什么样？**
    假设词表大小是 50,000 个词，模型的维度（Hidden Size）是 4,096。
    这个矩阵就是一个 `[50000 x 4096]` 的巨型表格。每一行都代表一个词的"语义坐标"。

*   **发生了什么？**
    模型拿到上一步的 ID `[254, 3301, 1209]`，然后去这个矩阵里"抽屉取物"：
    *   去第 254 行，把那一行 4096 个数字复制出来。
    *   去第 3301 行，把那一行 4096 个数字复制出来。
    *   ...

*   **结果：**
    现在的输入变成了一个 `[3, 4096]` 的浮点数矩阵。
    *   `254` 变成了 `[0.1, -0.9, 0.05 ...]` （代表"我"的语义向量）
    *   `3301` 变成了 `[0.8, 0.2, -0.3 ...]` （代表"爱"的语义向量）

> **关键点：** 这个 Embedding 矩阵的参数（那些浮点数），是和模型其他部分的参数**一起训练出来的**。所以 Base Model 自己就懂得如何把 ID 转换成它能理解的向量。

---

## 位置编码（Positional Encoding）— 加上顺序信息

只有 Embedding 是不够的，因为 Transformer 架构并行处理，它默认不知道"我爱你"和"你爱我"的区别。

在进入真正的深层网络之前，模型会把表示位置的向量（Position Vector）**加**到刚才的词向量上。
*   `最终向量 = 词义向量 + 位置向量`

### 不同的位置编码方法

随着技术发展，出现了多种位置编码方案：

*   **绝对位置编码（Absolute Positional Encoding）：** 原始Transformer使用的方法，使用正弦和余弦函数生成固定的位置向量。每个位置都有一个固定的编码。
*   **可学习位置编码（Learned Positional Encoding）：** GPT等模型使用，位置向量是训练出来的参数，不是固定的数学函数。
*   **相对位置编码（Relative Positional Encoding）：** 不编码绝对位置，而是编码词与词之间的相对距离。
*   **RoPE（Rotary Position Embedding）：** LLaMA等新模型使用，通过旋转变换注入位置信息，在长文本处理上表现更好。

不同的位置编码方法会影响模型对长文本的理解能力和上下文窗口的最大长度。

此时，数据正式准备好，可以进入 Transformer 的核心层进行"矩阵计算"了。

---

## Transformer 内部计算 — 预测下一个字

现在，数据流经几十层 Transformer Block（如 Self-Attention 和 MLP）。

每一层都在做大量的**矩阵乘法（Matrix Multiplication）**：
1.  **Attention（注意力）：** 计算词与词之间的关系。比如看到"爱"，模型会回头看"我"和"AI"，通过矩阵运算算出它们之间的关联强度。
2.  **MLP（前馈网络）：** 这一步就是我们在上一条回答中提到的"记忆提取"。模型根据上下文，激活特定的神经元。

### 上下文窗口限制

需要注意的是，模型并不能处理无限长的文本。每个模型都有一个 **上下文窗口（Context Window）** 限制：

*   **GPT-3.5：** 4,096 tokens（约3,000个中文字）
*   **GPT-4：** 8,192 tokens 或 32,768 tokens（取决于版本）
*   **Claude 3：** 200,000 tokens（约15万个中文字）
*   **Gemini 1.5 Pro：** 1,000,000 tokens

这个限制来自于Attention机制的计算复杂度——对于 N 个token，需要计算 N×N 的关系矩阵，内存和计算量会快速增长。所以如果输入超过这个长度，模型要么会截断，要么会报错。

**最终输出：**
经过几十层计算后，最后一个 token（也就是"AI"这个位置）会输出一个新的向量。这个向量包含了"我爱AI"这整句话的上下文含义。

---

## Unembedding（反向映射）— 向量变回字
最后一步，模型要把脑子里的"语义向量"变回人类能看的字。

1.  **投影：** 最后的向量会乘以一个巨大的矩阵（通常就是 Embedding 矩阵的转置，或者叫 Head）。这个操作会把向量维度从 4096 映射回 50,000（词表大小）。
2.  **Logits：** 此时，你会得到 50,000 个分数（Logits）。
    *   "吗"的分数：5.2
    *   "。"的分数：12.8
    *   "的"的分数：-3.0
3.  **Softmax：** 这些分数被转化成概率。
    *   "。"的概率：90%
    *   "吗"的概率：5%
4.  **采样：** 模型（或者说生成策略）选择概率最高的那个 ID，比如对应的是"。"。
5.  **解码：** Tokenizer 把 ID 换回文字 "。"。

### 采样策略 — 控制创造力

第4步的"采样"其实有很多种策略，它们会极大地影响模型的输出风格：

*   **Greedy Sampling（贪心采样）：** 永远选择概率最高的词。
    *   优点：输出稳定、确定
    *   缺点：可能重复、缺乏创造力

*   **Temperature（温度参数）：** 在计算概率之前，调整 Logits 的分布。
    *   `Temperature = 1.0`：原始分布
    *   `Temperature < 1.0`（如0.5）：让高概率的词更高，输出更确定
    *   `Temperature > 1.0`（如1.5）：让分布更平坦，输出更随机、更有创造力
    
    就像调节咖啡的温度——温度越高，味道越不稳定！

*   **Top-K Sampling：** 只从概率最高的 K 个词中随机选择。
    *   例如 `K=50`，只考虑前50个最可能的词
    *   避免选到那些概率极低的奇怪词

*   **Top-P Sampling（Nucleus Sampling）：** 选择累积概率达到 P 的最小词集合。
    *   例如 `P=0.9`，选择那些加起来占90%概率的词
    *   优点：动态调整候选词数量，更灵活

你在ChatGPT的设置中调整"温度"或"创造力"参数时，实际上就是在控制这些采样策略！

---

## 总结：完整流程

如果你输入"你好"：

1.  **Text:** "你好"
2.  **Tokenizer:** 查字典 -> `[ID: 101, ID: 204]`
3.  **Internal Embedding Layer:** 查内部矩阵 -> `[[0.1, ...], [0.9, ...]]` (变成向量)
4.  **Positional Encoding:** 加上位置信息 -> 最终输入向量
5.  **Transformer Layers:** 几十层的矩阵乘法 (混合上下文，提取记忆) -> `[Context Vector]`
6.  **Output Head:** 向量乘回词表大小 -> `[Logits]` (5万个词的分数)
7.  **Sampling Strategy:** 根据采样策略（如Top-P、Temperature）选择 ID -> `ID: 305`
8.  **Tokenizer:** 查字典 `ID: 305` -> "！"
